from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

# Base URL of the site to check
base_url = "https://www.protrainings.com"

# Set of URLs to check (to avoid duplicates)
urls_to_check = {base_url}

# Set of URLs already checked
checked_urls = set()

# Set up the Selenium WebDriver with headless Chrome
chrome_options = Options()
chrome_options.add_argument("--headless")
driver = webdriver.Chrome(options=chrome_options)

def check_seo_tags(page_url):
    try:
        driver.get(page_url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        errors = []

        # Checking Title Tag
        title_tag = soup.title
        if not title_tag or not title_tag.string.strip():
            errors.append("Missing or empty title tag.")
        elif len(title_tag.string.strip()) > 60:
            errors.append("Title tag too long.")

        # Checking Meta Description
        meta_description = soup.find("meta", attrs={"name": "description"})
        if not meta_description or not meta_description.get('content', '').strip():
            errors.append("Missing or empty meta description.")
        elif len(meta_description.get('content', '')) > 160:
            errors.append("Meta description too long.")

        # Checking Canonical Link
        canonical_link = soup.find("link", rel="canonical")
        if not canonical_link or not canonical_link.get('href', '').strip():
            errors.append("Missing or incorrect canonical link.")

        # Checking H1 Tags
        h1_tags = soup.find_all('h1')
        if not h1_tags:
            errors.append("No H1 tags found.")
        if len(h1_tags) > 1:
            errors.append("Multiple H1 tags found.")

        # Checking Alt Text for Images
        images = soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        if images_without_alt:
            errors.append(f"{len(images_without_alt)} images missing alt text.")

        return {
            "URL": page_url,
            "Title": title_tag.string.strip() if title_tag else "",
            "Meta Description": meta_description['content'] if meta_description else "",
            "Canonical Link": canonical_link['href'] if canonical_link else "",
            "H1 Tags": [h1.text.strip() for h1 in h1_tags],
            "Image Alt Texts": [img['alt'] for img in images if 'alt' in img.attrs],
            "Errors": errors
        }
    except Exception as e:
        print(f"Error checking SEO tags on {page_url}: {e}")
        return None

def crawl_site(page_url):
    try:
        driver.get(page_url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        for link in soup.find_all('a', href=True):
            href = link['href']
            if not href.startswith('http'):
                href = urljoin(page_url, href)
            if urlparse(href).netloc == urlparse(base_url).netloc and href not in checked_urls:
                urls_to_check.add(href)
    except Exception as e:
        print(f"Error crawling {page_url}: {e}")

while urls_to_check:
    url = urls_to_check.pop()
    if url not in checked_urls:
        print(f"Checking {url}...")
        seo_tags = check_seo_tags(url)
        if seo_tags:
            print(f"SEO Tags on {url}: {seo_tags}")
        else:
            print(f"Failed to retrieve SEO tags on {url}")
        crawl_site(url)  # Find more links to check
        checked_urls.add(url)
        time.sleep(1)  # Delay to avoid spam protection

driver.quit()
print("Finished checking all pages.")
