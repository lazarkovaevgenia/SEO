from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import pandas as pd

# Base URL of the site to check
base_url = "https://website.com/"

# Set of URLs to check (to avoid duplicates)
urls_to_check = {base_url}

# Set of URLs already checked
checked_urls = set()

# Initialize DataFrame to store results
columns = ["URL", "Title", "Meta Description", "Canonical Link", "H1 Tags", "Image Alt Texts",
           "Title Errors", "Meta Description Errors", "Canonical Link Errors", "H1 Tags Errors", "Image Alt Texts Errors"]
df = pd.DataFrame(columns=columns)

# Set up the Selenium WebDriver with headless Chrome
chrome_options = Options()
chrome_options.add_argument("--headless")
driver = webdriver.Chrome(options=chrome_options)

def check_seo_tags(page_url):
    try:
        driver.get(page_url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Initialize error lists
        title_errors = []
        meta_description_errors = []
        canonical_link_errors = []
        h1_tags_errors = []
        image_alt_text_errors = []

        # Checking Title Tag
        title_tag = soup.title
        if not title_tag or not title_tag.string.strip():
            title_errors.append("Missing or empty title tag.")
        elif len(title_tag.string.strip()) > 60:
            title_errors.append("Title tag too long.")

        # Checking Meta Description
        meta_description = soup.find("meta", attrs={"name": "description"})
        if not meta_description or not meta_description.get('content', '').strip():
            meta_description_errors.append("Missing or empty meta description.")
        elif len(meta_description.get('content', '')) > 160:
            meta_description_errors.append("Meta description too long.")

        # Checking Canonical Link
        canonical_link = soup.find("link", rel="canonical")
        if not canonical_link or not canonical_link.get('href', '').strip():
            canonical_link_errors.append("Missing or incorrect canonical link.")

        # Checking H1 Tags
        h1_tags = soup.find_all('h1')
        if not h1_tags:
            h1_tags_errors.append("No H1 tags found.")
        if len(h1_tags) > 1:
            h1_tags_errors.append("Multiple H1 tags found.")

        # Checking Alt Text for Images
        images = soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        if images_without_alt:
            image_alt_text_errors.append(f"{len(images_without_alt)} images missing alt text.")

        seo_data = {
            "URL": page_url,
            "Title": title_tag.string.strip() if title_tag else "",
            "Meta Description": meta_description['content'] if meta_description else "",
            "Canonical Link": canonical_link['href'] if canonical_link else "",
            "H1 Tags": [h1.text.strip() for h1 in h1_tags],
            "Image Alt Texts": [img['alt'] for img in images if 'alt' in img.attrs],
            "Title Errors": ', '.join(title_errors),
            "Meta Description Errors": ', '.join(meta_description_errors),
            "Canonical Link Errors": ', '.join(canonical_link_errors),
            "H1 Tags Errors": ', '.join(h1_tags_errors),
            "Image Alt Texts Errors": ', '.join(image_alt_text_errors)
        }
        return seo_data
    except Exception as e:
        print(f"Error checking SEO tags on {page_url}: {e}")
        return None

def crawl_site(page_url):
    try:
        driver.get(page_url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        for link in soup.find_all('a', href=True):
            href = link['href']
            if not href.startswith('http'):
                href = urljoin(page_url, href)
            if urlparse(href).netloc == urlparse(base_url).netloc and href not in checked_urls:
                urls_to_check.add(href)
    except Exception as e:
        print(f"Error crawling {page_url}: {e}")

while urls_to_check:
    url = urls_to_check.pop()
    if url not in checked_urls:
        print(f"Checking {url}...")
        seo_tags = check_seo_tags(url)
        if seo_tags:
            df = pd.concat([df, pd.DataFrame([seo_tags])], ignore_index=True)
        else:
            print(f"Failed to retrieve SEO tags on {url}")
        crawl_site(url)  # Find more links to check
        checked_urls.add(url)
        time.sleep(1)  # Delay to avoid spam protection

driver.quit()
print("Finished checking all pages.")

# Save results to Excel file
df.to_excel('seo_audit_results.xlsx', index=False)
